{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The aim of this assignment is to use Ensemble Learning to solve a problem. In this way, it is aimed to understand the benefits of Ensemble Learning and to teach the usage details of different ensemble approaches with the help of ScikitLearn library. \n",
    "\n",
    "The following methods will be implemented within the scope of this assignment. These are:\n",
    "- Voting Classifier (hard & soft voting) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate moon dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Voting classifier in Scikit-Learn\n",
    "- Train voting classifiers in Scikit-Learn, composed of at least three diverse classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = RandomForestClassifier(random_state=42)\n",
    "clf3 = SVC(probability=True, random_state=42)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "# define the hard voting classifier\n",
    "voting_clf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='hard')\n",
    "voting_clf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
    "\n",
    "# train the voting classifier using X_train, y_train\n",
    "voting_clf1.fit(X_train, y_train)\n",
    "voting_clf2.fit(X_train, y_train)\n",
    "\n",
    "#write obtained individual classifiers. Compare them with \"Voting Classifiers (hard and soft)\" for \"X_test/y_test\" data\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (clf1, clf2, clf3, voting_clf1, voting_clf2):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging/Pasting\n",
    "- One way to get a diverse set of classifiers is to use very different training algorithms\n",
    "- Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set\n",
    "    - When sampling is performed with replacement, this method is called ***bagging*** \n",
    "        - short for ***bootstrap aggregating***\n",
    "    - When sampling is performed without replacement, it is called pasting\n",
    "- Both bagging and pasting allow training instances to be sampled several times across multiple predictors\n",
    "- Only bagging allows training instances to be sampled several times for the same predictor\n",
    "- Predictors can all be trained in parallel, via different CPU cores or even different servers.\n",
    "- Similarly, predictions can be made in parallel.\n",
    "- This is one of the reasons why bagging and pasting are such popular methods: they scale very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class \n",
    "#  \n",
    "#   (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). \n",
    "# The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions \n",
    "#   (â€“1 tells Scikit-Learn to use all available cores):\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Write Bagging Classifier and train it using ensemble of 500 Decision Tree classifiers, \n",
    "#  each trained on 100 training instances randomly sampled from the training set with replacement \n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "bag_clf = BaggingClassifier(base_estimator=tree_clf, n_estimators=500,\n",
    "                            bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# bagging accuracy score\n",
    "# Using X_test dataset calculate/print Bagging Accuracy Score (using accuracy_score metric)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "bag_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging accuracy:\", bag_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy: 0.856\n"
     ]
    }
   ],
   "source": [
    "# Without bagging accuracy score\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate \"Tree Classifier\" accuracy score\n",
    "# Using X_test dataset calculate/print Tree Classifier Accuracy Score (using accuracy_score metric)\n",
    "\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Decision tree accuracy:\", tree_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Out-of-Bag evaluation\n",
    "- With bagging, some instances may be sampled several times for any given predictor, \n",
    " while others may not be sampled at all. \n",
    "- By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), \n",
    " where m is the size of the training set\n",
    "- Only about 60% of the training instances are sampled on average for each predictor\n",
    "- The remaining 40% of the training instances that are not sampled are called out-of-bag (oob) instances\n",
    "- Since a predictor never sees the oob instances during training, it can be evaluated on these instances, \n",
    "without the need for a separate validation set or cross-validation\n",
    "- In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier\n",
    " to request an automatic oob evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB score: 0.896\n",
      "OOB accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "#write your out-of-bag (oob) classifier and train it. \n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "oob_bag_clf = BaggingClassifier(base_estimator=tree_clf, n_estimators=500,\n",
    "                                bootstrap=True, oob_score=True, n_jobs=-1, random_state=42)\n",
    "oob_bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# According to this oob evaluation print your oob score for test dataset \n",
    "oob_bag_clf.oob_score_\n",
    "print(\"OOB score:\", oob_bag_clf.oob_score_)\n",
    "\n",
    "# Calculate oob bagging accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = oob_bag_clf.predict(X_test)\n",
    "oob_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"OOB accuracy:\", oob_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Random Forests\n",
    "- Random Forest is an ensemble of Decision Trees\n",
    "- Generally trained via the bagging method typically with max_samples set to the size of the training set\n",
    "- Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, \n",
    "you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees\n",
    "\n",
    "- Train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.912\n",
      "Accuracy:  0.912\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Write your RandomForestClassifier classifier using sklearn RandomForestClassifier class and train it. \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate/print the prediction accuracy of the rnd_clf for Test Data\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Write your RandomForestClassifier classifier using BaggingClassifier equivalent and train it.(estimator=500, leafnode=16) \n",
    "bag_rnd_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_leaf_nodes=16), n_estimators=500, n_jobs=-1\n",
    ")\n",
    "bag_rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate/print the prediction accuracy of the bag_rnd_clf for Test Data\n",
    "y_pred = bag_rnd_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Your Novel Ensemble Model \n",
    "- Write your customized Ensemble Classifier written by yourself \n",
    "- Try to get the highest score for the same dataset\n",
    "- There no limit. There is no specific constraints. Note that the classifier you write should be an Ensemble Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Write your Ensemble Classifier and train it. \n",
    "log_clf = LogisticRegression()\n",
    "svm_clf = SVC()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "my_ensemble_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svm_clf), ('dt', dt_clf)], voting='hard'\n",
    ")\n",
    "my_ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = my_ensemble_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
